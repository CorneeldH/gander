---
title: "Getting started with gander"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started with gander}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

gander is a high performance and low friction chat experience for data scientists in RStudio and Positron. The package supplies an addin that can be registered to a keybinding. Once that's done, just trigger the addin while you're working on documents and type a request; along with your request, the addin will automatically inject context about the document you're working in and the objects in your R environment.

```{r}
#| label: gander-demo
#| echo: false
knitr::include_url("https://github.com/user-attachments/assets/8d28a2c8-9f8c-47eb-940d-9f1586d2e9bb", height = "400px")
```

This interface excels beyond a typical LLM chat in two notable ways:

* **Higher-performance**: Including information about your R environment and relevant code ensures that the model has all of the context it needs to supply an answer that's as accurate as possible _and_ aligns with your taste and style preferences in that it can mirror the code you've written already.

* **Lower-friction**: gander eliminates the need to copy and paste relevant code into the chat window and its result back into your document. There's also no need to type anything other than your request, e.g. descriptions of the style you'd like the response to take or bits of exposition like "Here's some additional context:". Further, iterating on pieces of content is easier in that the model's output will automatically be selected after it's streamed into your document; to iterate on a response, just trigger the addin again.

## Choosing a model

The gander addin supports any model supported by [ellmer](https://ellmer.tidyverse.org/). When choosing a model for use with gander, you'll want to the use the most performant model possible that satisfies your privacy needs; gander automatically passes along your code and the objects in your R environment to your chosen model, so it's especially important to consider data privacy when using LLMs with gander.

gander uses the `.gander_fn` and `.gander_args` options to configure which model powers the addin. `.gander_fn` is the name of an ellmer `chat_*()` function as a string, and `.gander_args` is a list of arguments to pass to that function. For example, to use OpenAI's GPT-4o-mini, you might write `options(.gander_fn = "chat_openai", .gander_args = list(model = "gpt-4o-mini"))`. Paste that code in your `.Rprofile` via `usethis::edit_r_profile()` to always use the same model every time you start an R session.

If you're using ellmer inside a organization, you'll be limited to what your IT department allows, which is likely to be one provided by a big cloud provider, e.g. `chat_azure()`, `chat_bedrock()`, `chat_databricks()`, or `chat_snowflake()`. If you're using ellmer for your own exploration, you'll have a lot more freedom, so we have a few recommendations to help you get started:

- As of early 2025, Anthropic's Claude Sonnet 3.5 is a very powerful model for code assistance, and thus `chat_claude()` is the default model supported by gander. If you want to use Claude, you'll need to register an [API key](https://console.anthropic.com/) to the environment variable `ANTHROPIC_API_KEY`. No need to set the `.gander_*` options in this case.

* Regarding OpenAI's models, `chat_openai()` defaults to **GPT-4o**, but you can use `model = "gpt-4o-mini"` for a cheaper, lower-quality model, or `model = "o1-mini"` for more complex reasoning; to use an OpenAI model, you'll need to set the options `options(.gander_fn = "chat_openai", .gander_args = list(model = "gpt-4o-mini"))` and register your OpenAI API key with the `OPENAI_API_KEY` environment variable.

- You can use a local model with `chat_ollama()`, which uses [Ollama](https://ollama.com) and allows you to run models on your own computer. While the biggest models you can run locally aren't as good as the state of the art hosted models, they don't share your data and are effectively free. To use an ollama model, run the model locally and then set `options(.gander_fn = "chat_ollama", .gander_args = list(model = "model-name"))`.

<!-- ## What is gander actually doing? -->

<!-- call out `gander_peek()` -->
